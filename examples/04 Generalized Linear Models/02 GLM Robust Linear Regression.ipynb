{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLM: Robust Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tutorial is the second of a three-part series on Bayesian generalized linear models (GLMs), that first appeared on [Thomas Wiecki’s blog](https://twiecki.io/):\n",
    "\n",
    "1. [Linear Regression](https://www.pymc.io/projects/docs/en/stable/learn/core_notebooks/GLM_linear.html#glm-linear)\n",
    "1. [Robust Linear Regression](https://www.pymc.io/projects/examples/en/latest/generalized_linear_models/GLM-robust.html#glm-robust)\n",
    "1. [Hierarchical Linear Regression](https://www.pymc.io/projects/examples/en/latest/generalized_linear_models/multilevel_modeling.html#glm-hierarchical)\n",
    "\n",
    "In this blog post I will write about:\n",
    "\n",
    "* How a few outliers can largely affect the fit of linear regression models.\n",
    "* How replacing the normal likelihood with Student T distribution produces robust regression.\n",
    "\n",
    "In the [linear regression tutorial](https://www.pymc.io/projects/docs/en/stable/learn/core_notebooks/GLM_linear.html#glm-linear) I described how minimizing the squared distance of the regression line is the same as maximizing the likelihood of a Normal distribution with the mean coming from the regression line. This latter probabilistic expression allows us to easily formulate a Bayesian linear regression model.\n",
    "\n",
    "This worked splendidly on simulated data. The problem with simulated data though is that it’s, well, simulated. In the real world things tend to get more messy and assumptions like normality are easily violated by a few outliers.\n",
    "\n",
    "Lets see what happens if we add some outliers to our simulated data from the last post.\n",
    "\n",
    "First, let’s import our modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc as pm\n",
    "import pytensor\n",
    "import pytensor.tensor as pt\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 8927\n",
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "az.style.use(\"arviz-darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create some toy data but also add some outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 100\n",
    "true_intercept = 1\n",
    "true_slope = 2\n",
    "\n",
    "x = np.linspace(0, 1, size)\n",
    "# y = a + b*x\n",
    "true_regression_line = true_intercept + true_slope * x\n",
    "# add noise\n",
    "y = true_regression_line + rng.normal(scale=0.5, size=size)\n",
    "\n",
    "# Add outliers\n",
    "x_out = np.append(x, [0.1, 0.15, 0.2])\n",
    "y_out = np.append(y, [8, 6, 9])\n",
    "\n",
    "data = pd.DataFrame(dict(x=x_out, y=y_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the data together with the true regression line (the three points in the upper left corner are the outliers we added)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7, 5))\n",
    "ax = fig.add_subplot(111, xlabel=\"x\", ylabel=\"y\", title=\"Generated data and underlying model\")\n",
    "ax.plot(x_out, y_out, \"x\", label=\"sampled data\")\n",
    "ax.plot(x, true_regression_line, label=\"true regression line\", lw=2.0)\n",
    "plt.legend(loc=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robust Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see what happens if we estimate our Bayesian linear regression model by fitting a regression model with a normal likelihood. Note that the bambi library provides an easy to use such that an equivalent model can be built using one line of code. A version of this same notebook using Bambi is available at [bambi’s docs](https://bambinos.github.io/bambi/notebooks/t_regression.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    xdata = pm.ConstantData(\"x\", x_out, dims=\"obs_id\")\n",
    "\n",
    "    # define priors\n",
    "    intercept = pm.Normal(\"intercept\", mu=0, sigma=1)\n",
    "    slope = pm.Normal(\"slope\", mu=0, sigma=1)\n",
    "    sigma = pm.HalfCauchy(\"sigma\", beta=10)\n",
    "\n",
    "    mu = pm.Deterministic(\"mu\", intercept + slope * xdata, dims=\"obs_id\")\n",
    "\n",
    "    # define likelihood\n",
    "    likelihood = pm.Normal(\"y\", mu=mu, sigma=sigma, observed=y_out, dims=\"obs_id\")\n",
    "\n",
    "    # inference\n",
    "    trace = pm.sample(tune=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the fit, the code below calculates the posterior predictive regression lines by taking regression parameters from the posterior distribution and plots a regression line for 20 of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post = az.extract(trace, num_samples=20)\n",
    "x_plot = xr.DataArray(np.linspace(x_out.min(), x_out.max(), 100), dims=\"plot_id\")\n",
    "lines = post[\"intercept\"] + post[\"slope\"] * x_plot\n",
    "\n",
    "plt.scatter(x_out, y_out, label=\"data\")\n",
    "plt.plot(x_plot, lines.transpose(), alpha=0.4, color=\"C1\")\n",
    "plt.plot(x, true_regression_line, label=\"True regression line\", lw=3.0, c=\"C2\")\n",
    "plt.legend(loc=0)\n",
    "plt.title(\"Posterior predictive for normal likelihood\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the fit is quite skewed and we have a fair amount of uncertainty in our estimate as indicated by the wide range of different posterior predictive regression lines. Why is this? The reason is that the normal distribution does not have a lot of mass in the tails and consequently, an outlier will affect the fit strongly.\n",
    "\n",
    "A Frequentist would estimate a [Robust Regression](http://en.wikipedia.org/wiki/Robust_regression) and use a non-quadratic distance measure to evaluate the fit.\n",
    "\n",
    "But what’s a Bayesian to do? Since the problem is the light tails of the Normal distribution we can instead assume that our data is not normally distributed but instead distributed according to the [Student T distribution](http://en.wikipedia.org/wiki/Student%27s_t-distribution) which has heavier tails as shown next [[Kruschke, 2014](https://www.pymc.io/projects/examples/en/latest/introductory/api_quickstart.html#id56), [Gelman et al., 2013](https://www.pymc.io/projects/examples/en/latest/generalized_linear_models/GLM-truncated-censored-regression.html#id34)].\n",
    "\n",
    "Lets look at those two distributions to get a feel for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_dist = pm.Normal.dist(mu=0, sigma=1)\n",
    "t_dist = pm.StudentT.dist(mu=0, lam=1, nu=1)\n",
    "x_eval = np.linspace(-8, 8, 300)\n",
    "plt.plot(x_eval, pm.math.exp(pm.logp(normal_dist, x_eval)).eval(), label=\"Normal\", lw=2.0)\n",
    "plt.plot(x_eval, pm.math.exp(pm.logp(t_dist, x_eval)).eval(), label=\"Student T\", lw=2.0)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"Probability density\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the probability of values far away from the mean (0 in this case) are much more likely under the ```T``` distribution than under the Normal distribution.\n",
    "\n",
    "Below is a PyMC model, with the ```likelihood``` term following a ```StudentT``` distribution with $\\nu = 3$\n",
    " degrees of freedom, opposed to the ```Normal``` distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as robust_model:\n",
    "    xdata = pm.ConstantData(\"x\", x_out, dims=\"obs_id\")\n",
    "\n",
    "    # define priors\n",
    "    intercept = pm.Normal(\"intercept\", mu=0, sigma=1)\n",
    "    slope = pm.Normal(\"slope\", mu=0, sigma=1)\n",
    "    sigma = pm.HalfCauchy(\"sigma\", beta=10)\n",
    "\n",
    "    mu = pm.Deterministic(\"mu\", intercept + slope * xdata, dims=\"obs_id\")\n",
    "\n",
    "    # define likelihood\n",
    "    likelihood = pm.StudentT(\"y\", mu=mu, sigma=sigma, nu=3, observed=y_out, dims=\"obs_id\")\n",
    "\n",
    "    # inference\n",
    "    robust_trace = pm.sample(tune=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robust_post = az.extract(robust_trace, num_samples=20)\n",
    "x_plot = xr.DataArray(np.linspace(x_out.min(), x_out.max(), 100), dims=\"plot_id\")\n",
    "robust_lines = robust_post[\"intercept\"] + robust_post[\"slope\"] * x_plot\n",
    "\n",
    "plt.scatter(x_out, y_out, label=\"data\")\n",
    "plt.plot(x_plot, robust_lines.transpose(), alpha=0.4, color=\"C1\")\n",
    "plt.plot(x, true_regression_line, label=\"True regression line\", lw=3.0, c=\"C2\")\n",
    "plt.legend(loc=0)\n",
    "plt.title(\"Posterior predictive for Student-T likelihood\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There, much better! The outliers are barely influencing our estimation at all because our likelihood function assumes that outliers are much more probable than under the Normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* By changing the likelihood from a Normal distribution to a Student T distribution – which has more mass in the tails – we can perform Robust Regression.\n",
    "\n",
    "Extensions:\n",
    "\n",
    "* The Student-T distribution has, besides the mean and variance, a third parameter called degrees of freedom that describes how much mass should be put into the tails. Here it is set to 1 which gives maximum mass to the tails (setting this to infinity results in a Normal distribution!). One could easily place a prior on this rather than fixing it which I leave as an exercise for the reader ;).\n",
    "\n",
    "* T distributions can be used as priors as well. See [A Primer on Bayesian Methods for Multilevel Modeling](https://www.pymc.io/projects/examples/en/latest/generalized_linear_models/multilevel_modeling.html#glm-hierarchical)\n",
    "\n",
    "* How do we test if our data is normal or violates that assumption in an important way? Check out this great blog post, [Probably Overthinking It](http://allendowney.blogspot.com/2013/08/are-my-data-normal.html), by Allen Downey."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Adapted from Thomas Wiecki’s blog\n",
    "* Updated by @fonnesbeck in September 2016 (pymc#1378)\n",
    "* Updated by @chiral-carbon in August 2021 (pymc-examples#205)\n",
    "* Updated by Conor Hassan, Igor Kuvychko, Reshama Shaikh and Oriol Abril Pla in 2022\n",
    "* Rerun using PyMC v5, by Reshama Shaikh, January 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. John Kruschke. Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan. Academic Press, 2014.\n",
    "1. Andrew Gelman, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B. Rubin. Bayesian Data Analysis. Chapman and Hall/CRC, 2013."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -n -u -v -iv -w -p xarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License notice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the notebooks in this example gallery are provided under the [MIT License](https://github.com/pymc-devs/pymc-examples/blob/main/LICENSE) which allows modification, and redistribution for any use provided the copyright and license notices are preserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citing PyMC examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To cite this notebook, use the DOI provided by Zenodo for the pymc-examples repository.\n",
    "\n",
    "Important\n",
    "\n",
    "Many notebooks are adapted from other sources: blogs, books… In such cases you should cite the original source as well.\n",
    "\n",
    "Also remember to cite the relevant libraries used by your code.\n",
    "\n",
    "Here is an citation template in bibtex:\n",
    "\n",
    "```py\n",
    "@incollection{citekey,\n",
    "  author    = \"<notebook authors, see above>\",\n",
    "  title     = \"<notebook title>\",\n",
    "  editor    = \"PyMC Team\",\n",
    "  booktitle = \"PyMC examples\",\n",
    "  doi       = \"10.5281/zenodo.5654871\"\n",
    "}\n",
    "```\n",
    "which once rendered could look like:\n",
    "\n",
    "Thomas Wiecki , Chris Fonnesbeck , Abhipsha Das , Conor Hassan , Igor Kuvychko , Reshama Shaikh , Oriol Abril Pla . \"GLM: Robust Linear Regression\". In: PyMC Examples. Ed. by PyMC Team. DOI: 10.5281/zenodo.5654871"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
