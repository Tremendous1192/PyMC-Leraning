{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Missing Data Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'jax'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimize\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlines\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Line2D\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpymc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msampling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjax\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sample_blackjax_nuts, sample_numpyro_nuts\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m multivariate_normal\n",
      "File \u001b[1;32mc:\\Users\\treme\\.conda\\envs\\pymc_env\\Lib\\site-packages\\pymc\\sampling\\jax.py:24\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Literal\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01marviz\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01maz\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mjnp\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'jax'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import arviz as az\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc as pm\n",
    "import scipy.optimize\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "from pymc.sampling.jax import sample_blackjax_nuts, sample_numpyro_nuts\n",
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Imputation and Degrees of Missing-ness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analysis of data with missing values is a gateway into the study of causal inference.\n",
    "\n",
    "One of the key features of any analysis plagued by missing data is the assumption which governs the nature of the missing-ness i.e. what is the reason for gaps in our data?\n",
    "Can we ignore them?\n",
    "Should we worry about why?\n",
    "In this notebook we’ll see an example of how to handle missing data using maximum likelihood estimation and bayesian imputation techniques.\n",
    "This will open up questions about the assumptions governing inference in the presence of missing data, and inference in counterfactual cases.\n",
    "\n",
    "We will make the discussion concrete by considering an example analysis of an employee satisfaction survey and how different work conditions contribute to the responses and non-responses we see in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'  # high resolution figures\n",
    "az.style.use(\"arviz-darkgrid\")\n",
    "rng = np.random.default_rng(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data Taxonomy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rubin’s famous taxonomy breaks out the question into a choice of three fundamental options:\n",
    "\n",
    "* Missing Completely at Random (MCAR)\n",
    "* Missing at Random (MAR)\n",
    "* Missing Not at Random (MNAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these paradigms can be reduced to explicit definition in terms of the conditional probability regarding the pattern of missing data.\n",
    "The first pattern is the least concerning.\n",
    "The (MCAR) assumption states that the data are missing in a manner that is unrelated to both the observed and unobserved parts of the realised data.\n",
    "It is missing due to the haphazard circumstance of the world $\\phi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "P(M = 1 | Y_{obs}, Y_{miss}, \\phi)\n",
    "=\n",
    "P(M = 1 | \\phi)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "whereas the second pattern (MAR) allows that the reasons for missingness can be function of the observed data and circumstances of the world. Some times this is called a case of ignorable missingness because estimation can proceed in good faith on the basis of the observed data. There may be a loss of precision, but the inference should be sound."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "P(M = 1 | Y_{obs}, Y_{miss}, \\phi)\n",
    "=\n",
    "P(M = 1 | Y_{obs}, \\phi)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most nefarious sort of missing data is when the missingness is a function of something outside the observed data, and the equation cannot be reduced further.\n",
    "Efforts at imputation and estimation more generally may become more difficulty in this final case because of the risk of confounding.\n",
    "This is a case of non-ignorable missing-ness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "P(M = 1 | Y_{obs}, Y_{miss}, \\phi)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These assumptions are made before any analysis begins. They are inherently unverifiable.\n",
    "Your analysis will stand or fall depending on how plausible each assumption is in the context you seek to apply them.\n",
    "For example, an another type missing data results from systematic censoring as discussed in [Bayesian regression with truncated or censored data](https://www.pymc.io/projects/examples/en/latest/generalized_linear_models/GLM-truncated-censored-regression.html#glm-truncated-censored-regression).\n",
    "In such cases the reason for censoring governs the missing-ness pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Employee Satisfaction Surveys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll follow the presentation of Craig Enders’ Applied Missing Data Analysis Enders K [2022](https://www.pymc.io/projects/examples/en/latest/howto/Missing_Data_Imputation.html#id27) and work with employee satisifaction data set.\n",
    "The data set comprises of a few composite measures reporting employee working conditions and satisfactions.\n",
    "Of particular note are empowerment (```empower```), work satisfaction (```worksat```) and two composite survey scores recording the employees leadership climate (```climate```), and the relationship quality with their supervisor ```lmx```.\n",
    "\n",
    "The key question is what assumptions governs our patterns of missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_employee = pd.read_csv(\"../../data/employee.csv\")\n",
    "except FileNotFoundError:\n",
    "    df_employee = pd.read_csv(pm.get_data(\"employee.csv\"))\n",
    "df_employee.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage Missing\n",
    "df_employee[[\"worksat\", \"empower\", \"lmx\"]].isna().sum() / len(df_employee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patterns of missing Data\n",
    "df_employee[[\"worksat\", \"empower\", \"lmx\"]].isnull().drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 7))\n",
    "ax.hist(df_employee[\"empower\"], bins=30, ec=\"black\", color=\"cyan\", label=\"Empowerment\")\n",
    "ax.hist(df_employee[\"lmx\"], bins=30, ec=\"black\", color=\"yellow\", label=\"LMX\")\n",
    "ax.hist(df_employee[\"worksat\"], bins=30, ec=\"black\", color=\"green\", label=\"Work Satisfaction\")\n",
    "ax.set_title(\"Employee Satisfaction Survey Results\", fontsize=20)\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here the histograms of the employee metrics.\n",
    "It is the gaps in the data that we wish to impute to better understand the relationships between the variables and how gaps in one may be driven by values in the others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIML: Full Information Maximum Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method of handling missing data is not an imputation method.\n",
    "It uses maximum likelihood estimation to estimate the parameters of the multivariate normal distribution that could be best said to generate our observed data.\n",
    "It’s a little trickier than straight forward MLE approaches in that it respects the fact that we have missing data in our original data set, but fundamentally it’s the same idea.\n",
    "We want to optimize the parameters of our multivariate normal distribution to best fit the observed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The procedure works by partitioning the data into their patterns of “missing-ness” and treating each partition as contributing to the ultimate log-likelihood term that we want to maximise.\n",
    "We combine their contributions to estimate a fit for the multivariate normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_employee[[\"worksat\", \"empower\", \"lmx\"]]\n",
    "\n",
    "\n",
    "def split_data_by_missing_pattern(data):\n",
    "    # We want to extract our the pattern of missing-ness in our dataset\n",
    "    # and save each sub-set of our data in a structure that can be used to feed into a log-likelihood function\n",
    "    grouped_patterns = []\n",
    "    patterns = data.notnull().drop_duplicates().values\n",
    "    # A pattern is whether the values in each column e.g. [True, True, True] or [True, True, False]\n",
    "    observed = data.notnull()\n",
    "    for p in range(len(patterns)):\n",
    "        temp = observed[\n",
    "            (observed[\"worksat\"] == patterns[p][0])\n",
    "            & (observed[\"empower\"] == patterns[p][1])\n",
    "            & (observed[\"lmx\"] == patterns[p][2])\n",
    "        ]\n",
    "        grouped_patterns.append([patterns[p], temp.index, data.iloc[temp.index].dropna(axis=1)])\n",
    "\n",
    "    return grouped_patterns\n",
    "\n",
    "\n",
    "def reconstitute_params(params_vector, n_vars):\n",
    "    # Convenience numpy function to construct mirrored COV matrix\n",
    "    # From flattened params_vector\n",
    "    mus = params_vector[0:n_vars]\n",
    "    cov_flat = params_vector[n_vars:]\n",
    "    indices = np.tril_indices(n_vars)\n",
    "    cov = np.empty((n_vars, n_vars))\n",
    "    for i, j, c in zip(indices[0], indices[1], cov_flat):\n",
    "        cov[i, j] = c\n",
    "        cov[j, i] = c\n",
    "    cov = cov + 1e-25\n",
    "    return mus, cov\n",
    "\n",
    "\n",
    "def optimise_ll(flat_params, n_vars, grouped_patterns):\n",
    "    mus, cov = reconstitute_params(flat_params, n_vars)\n",
    "    # Check if COV is positive definite\n",
    "    if (np.linalg.eigvalsh(cov) < 0).any():\n",
    "        return 1e100\n",
    "    objval = 0.0\n",
    "    for obs_pattern, _, obs_data in grouped_patterns:\n",
    "        # This is the key (tricky) step because we're selecting the variables which pattern\n",
    "        # the full information set within each pattern of \"missing-ness\"\n",
    "        # e.g. when the observed pattern is [True, True, False] we want the first two variables\n",
    "        # of the mus vector and we want only the covariance relations between the relevant variables from the cov\n",
    "        # in the iteration.\n",
    "        obs_mus = mus[obs_pattern]\n",
    "        obs_cov = cov[obs_pattern][:, obs_pattern]\n",
    "        ll = np.sum(multivariate_normal(obs_mus, obs_cov).logpdf(obs_data))\n",
    "        objval = ll + objval\n",
    "    return -objval\n",
    "\n",
    "\n",
    "def estimate(data):\n",
    "    n_vars = data.shape[1]\n",
    "    # Initialise\n",
    "    mus0 = np.zeros(n_vars)\n",
    "    cov0 = np.eye(n_vars)\n",
    "    # Flatten params for optimiser\n",
    "    params0 = np.append(mus0, cov0[np.tril_indices(n_vars)])\n",
    "    # Process Data\n",
    "    grouped_patterns = split_data_by_missing_pattern(data)\n",
    "    # Run the Optimiser.\n",
    "    try:\n",
    "        result = scipy.optimize.minimize(\n",
    "            optimise_ll, params0, args=(n_vars, grouped_patterns), method=\"Powell\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "    mean, cov = reconstitute_params(result.x, n_vars)\n",
    "    return mean, cov\n",
    "\n",
    "\n",
    "fiml_mus, fiml_cov = estimate(data)\n",
    "\n",
    "\n",
    "print(\"Full information Maximum Likelihood Estimate Mu:\")\n",
    "display(pd.DataFrame(fiml_mus, index=data.columns).T)\n",
    "print(\"Full information Maximum Likelihood Estimate COV:\")\n",
    "pd.DataFrame(fiml_cov, columns=data.columns, index=data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from the Implied Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then sample from the implied distribution to estimate other features of interest and test against the observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mle_fit = multivariate_normal(fiml_mus, fiml_cov)\n",
    "mle_sample = mle_fit.rvs(10000)\n",
    "mle_sample = pd.DataFrame(mle_sample, columns=[\"worksat\", \"empower\", \"lmx\"])\n",
    "mle_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows us to compare the implied distributions against the observed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 7))\n",
    "ax.hist(\n",
    "    mle_sample[\"empower\"],\n",
    "    bins=30,\n",
    "    ec=\"black\",\n",
    "    color=\"cyan\",\n",
    "    alpha=0.2,\n",
    "    label=\"Inferred Empowerment\",\n",
    ")\n",
    "ax.hist(mle_sample[\"lmx\"], bins=30, ec=\"black\", color=\"yellow\", alpha=0.2, label=\"Inferred LMX\")\n",
    "ax.hist(\n",
    "    mle_sample[\"worksat\"],\n",
    "    bins=30,\n",
    "    ec=\"black\",\n",
    "    color=\"green\",\n",
    "    alpha=0.2,\n",
    "    label=\"Inferred Work Satisfaction\",\n",
    ")\n",
    "ax.hist(data[\"empower\"], bins=30, ec=\"black\", color=\"cyan\", label=\"Observed Empowerment\")\n",
    "ax.hist(data[\"lmx\"], bins=30, ec=\"black\", color=\"yellow\", label=\"Observed LMX\")\n",
    "ax.hist(data[\"worksat\"], bins=30, ec=\"black\", color=\"green\", label=\"Observed Work Satisfaction\")\n",
    "ax.set_title(\"Inferred from MLE fit: Employee Satisfaction Survey Results\", fontsize=20)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Correlation Between the Imputed Metrics Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also calculate other features of interest from our sample. For instance, we might want to know about the correlations between variables in question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(mle_sample.corr(), columns=data.columns, index=data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrapping Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may also want to validate the estimated parameters against bootstrapped samples under different speficiations of missing-ness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_200 = df_employee[[\"worksat\", \"empower\", \"lmx\"]].dropna().sample(200)\n",
    "data_200.reset_index(inplace=True, drop=True)\n",
    "\n",
    "\n",
    "sensitivity = {}\n",
    "n_missing = np.linspace(30, 100, 5)  # Change or alter the range as desired\n",
    "bootstrap_iterations = 100  # change to large number running a real analysis in this case\n",
    "for n in n_missing:\n",
    "    sensitivity[int(n)] = {}\n",
    "    sensitivity[int(n)][\"mus\"] = []\n",
    "    sensitivity[int(n)][\"cov\"] = []\n",
    "    for i in range(bootstrap_iterations):\n",
    "        temp = data_200.copy()\n",
    "        for m in range(int(n)):\n",
    "            i = random.choice(range(200))\n",
    "            j = random.choice(range(3))\n",
    "            temp.iloc[i, j] = np.nan\n",
    "        try:\n",
    "            fiml_mus, fiml_cov = estimate(temp)\n",
    "            sensitivity[int(n)][\"mus\"].append(fiml_mus)\n",
    "            sensitivity[int(n)][\"cov\"].append(fiml_cov)\n",
    "        except Exception as e:\n",
    "            next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we plot the maximum likelihood parameter estimates against various missing data regimes. This approach can be applied for any imputation methodology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(20, 7))\n",
    "for n in sensitivity.keys():\n",
    "    temp = pd.DataFrame(sensitivity[n][\"mus\"], columns=[\"worksat\", \"empower\", \"lmx\"])\n",
    "    for col, ax in zip(temp.columns, axs):\n",
    "        ax.hist(\n",
    "            temp[col], alpha=0.1, ec=\"black\", label=f\"Missing: {np.round(n/200, 2)}, Mean: {col}\"\n",
    "        )\n",
    "        ax.legend()\n",
    "        ax.set_title(f\"Bootstrap Distribution for Mean:\\n{col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 3, figsize=(20, 14))\n",
    "axs = axs.flatten()\n",
    "for n in sensitivity.keys():\n",
    "    length = len(sensitivity[n][\"cov\"])\n",
    "    temp = pd.DataFrame(\n",
    "        [sensitivity[n][\"cov\"][i][np.tril_indices(3)] for i in range(length)],\n",
    "        columns=[\n",
    "            \"var(worksat)\",\n",
    "            \"cov(worksat, empower)\",\n",
    "            \"var(empower)\",\n",
    "            \"cov(worksat, lmx)\",\n",
    "            \"cov(lmx, empower)\",\n",
    "            \"var(lmx)\",\n",
    "        ],\n",
    "    )\n",
    "    for col, ax in zip(temp.columns, axs):\n",
    "        ax.hist(\n",
    "            temp[col], alpha=0.1, ec=\"black\", label=f\"Missing: {np.round(n/200, 2)}, Mean: {col}\"\n",
    "        )\n",
    "        ax.legend()\n",
    "        ax.set_title(f\"Bootstrap Distribution for Expected:\\n{col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These plots show how under (MCAR) the parameter estimates of our multivariate normal distribution are quite robust to varying degrees of missing data. It’s an instructive exercise to attempt a similar simulation exercise under alternative missing data regimes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we’ll apply bayesian methods to the same problem. But here we’ll see direct imputation of the missing values using the posterior predictive distribution.\n",
    "The Bayesian approach to imputation is of a different flavour than we saw above.\n",
    "We’re not just learning parameters of the data generating distribution (although we are doing that too), the bayesian process directly imputes the missing values for specific missing entries through the process of MCMC sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytensor.tensor as pt\n",
    "\n",
    "with pm.Model() as model:\n",
    "    # Priors\n",
    "    mus = pm.Normal(\"mus\", 0, 1, size=3)\n",
    "    cov_flat_prior, _, _ = pm.LKJCholeskyCov(\"cov\", n=3, eta=1.0, sd_dist=pm.Exponential.dist(1))\n",
    "    # Create a vector of flat variables for the unobserved components of the MvNormal\n",
    "    x_unobs = pm.Uniform(\"x_unobs\", 0, 100, shape=(np.isnan(data.values).sum(),))\n",
    "\n",
    "    # Create the symbolic value of x, combining observed data and unobserved variables\n",
    "    x = pt.as_tensor(data.values)\n",
    "    x = pm.Deterministic(\"x\", pt.set_subtensor(x[np.isnan(data.values)], x_unobs))\n",
    "\n",
    "    # Add a Potential with the logp of the variable conditioned on `x`\n",
    "    pm.Potential(\"x_logp\", pm.logp(rv=pm.MvNormal.dist(mus, chol=cov_flat_prior), value=x))\n",
    "    idata = pm.sample_prior_predictive()\n",
    "    idata = pm.sample()\n",
    "    idata.extend(pm.sample(random_seed=120))\n",
    "    pm.sample_posterior_predictive(idata, extend_inferencedata=True)\n",
    "\n",
    "pm.model_to_graphviz(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_posterior(idata, var_names=[\"mus\", \"cov\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(idata, var_names=[\"mus\", \"cov\", \"x_unobs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_dims = data.shape\n",
    "imputed = data.values.flatten()\n",
    "imputed[np.isnan(imputed)] = az.summary(idata, var_names=[\"x_unobs\"])[\"mean\"].values\n",
    "imputed = imputed.reshape(imputed_dims[0], imputed_dims[1])\n",
    "imputed = pd.DataFrame(imputed, columns=[col + \"_imputed\" for col in data.columns])\n",
    "imputed.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(20, 7))\n",
    "axs = axs.flatten()\n",
    "for col, col_i, ax in zip(data.columns, imputed.columns, axs):\n",
    "    ax.hist(data[col], color=\"red\", label=col, ec=\"black\", bins=30)\n",
    "    ax.hist(imputed[col_i], color=\"cyan\", alpha=0.3, label=col_i, ec=\"black\", bins=30)\n",
    "    ax.legend()\n",
    "    ax.set_title(f\"Imputed Distribution and Observed for {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(az.summary(idata, var_names=[\"cov_corr\"])[\"mean\"].values.reshape(3, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results agree with the FIML approach above and the results reported in Ender’s Applied Missing Data Analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Imputation by Chained Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we’ve seen multivariate approaches to imputation which treat each of the variables in our dataset as a collection drawn from the same distribution.\n",
    "However, there is a more flexible approach which is often useful when there is a particular focal relationship that we’re interested in analysing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sticking with the employee data set we’ll examine here the relationship between ```lmx```, ```climate```, ```male``` and ```empower```, where our focus is on what drives empowerment.\n",
    "Recall that our gender variable ```male``` is fully specified and does not need to be imputed. So we have a joint distribution that can be decomposed:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "f(emp, lmx, climate, male)\n",
    "=\n",
    "f(emp | lmx, climate, male)\n",
    "\\cdot\n",
    "f(lmx | climate, male)\n",
    "\\cdot\n",
    "f(climate | male)\n",
    "\\cdot\n",
    "f(male)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which can be split out into individual regression equations or more generally component models for each required conditional model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "empower &= \\akpha _2 + \\beta _3 male + \\beta _4 climate + \\beta _5 lmx \\\\\n",
    "lmx &= \\alpha _1 + \\beta _ 1climate + \\beta _2 male \\\\\n",
    "climate &= \\alpha _0 + \\beta _0 male\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can impute each of these equations in turn saving the imputed data set and feeding it forward into the next modelling exercise.\n",
    "This adds a little complexity because some of the variables will occur twice. Once as a predictor in our focal regression and once and as likelihood term in their own component model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyMC Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw above we can use PyMC to impute the values of missing data by using a particular sampling distribution.\n",
    "In the case of chained equations this becomes a little trickier because we might want to use both the data for ```lmx``` as a regressor in one equation and observed data in our likelihood in another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also matters how we specify the sampling distribution that will be used to impute our missing data. We’ll show an example here where we use a uniform and normal sampling distribution alternatively for imputing the predictor terms in our in focal regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_employee[[\"lmx\", \"empower\", \"climate\", \"male\"]]\n",
    "lmx_mean = data[\"lmx\"].mean()\n",
    "lmx_min = data[\"lmx\"].min()\n",
    "lmx_max = data[\"lmx\"].max()\n",
    "lmx_sd = data[\"lmx\"].std()\n",
    "\n",
    "cli_mean = data[\"climate\"].mean()\n",
    "cli_min = data[\"climate\"].min()\n",
    "cli_max = data[\"climate\"].max()\n",
    "cli_sd = data[\"climate\"].std()\n",
    "\n",
    "\n",
    "priors = {\n",
    "    \"climate\": {\"normal\": [lmx_mean, lmx_sd, lmx_sd], \"uniform\": [lmx_min, lmx_max]},\n",
    "    \"lmx\": {\"normal\": [cli_mean, cli_sd, cli_sd], \"uniform\": [cli_min, cli_max]},\n",
    "}\n",
    "\n",
    "\n",
    "def make_model(priors, normal_pred_assumption=True):\n",
    "    coords = {\n",
    "        \"alpha_dim\": [\"lmx_imputed\", \"climate_imputed\", \"empower_imputed\"],\n",
    "        \"beta_dim\": [\n",
    "            \"lmxB_male\",\n",
    "            \"lmxB_climate\",\n",
    "            \"climateB_male\",\n",
    "            \"empB_male\",\n",
    "            \"empB_climate\",\n",
    "            \"empB_lmx\",\n",
    "        ],\n",
    "    }\n",
    "    with pm.Model(coords=coords) as model:\n",
    "        # Priors\n",
    "        beta = pm.Normal(\"beta\", 0, 1, size=6, dims=\"beta_dim\")\n",
    "        alpha = pm.Normal(\"alphas\", 10, 5, size=3, dims=\"alpha_dim\")\n",
    "        sigma = pm.HalfNormal(\"sigmas\", 5, size=3, dims=\"alpha_dim\")\n",
    "\n",
    "        if normal_pred_assumption:\n",
    "            mu_climate = pm.Normal(\n",
    "                \"mu_climate\", priors[\"climate\"][\"normal\"][0], priors[\"climate\"][\"normal\"][1]\n",
    "            )\n",
    "            sigma_climate = pm.HalfNormal(\"sigma_climate\", priors[\"climate\"][\"normal\"][2])\n",
    "            climate_pred = pm.Normal(\n",
    "                \"climate_pred\", mu_climate, sigma_climate, observed=data[\"climate\"].values\n",
    "            )\n",
    "        else:\n",
    "            climate_pred = pm.Uniform(\"climate_pred\", 0, 40, observed=data[\"climate\"].values)\n",
    "\n",
    "        if normal_pred_assumption:\n",
    "            mu_lmx = pm.Normal(\"mu_lmx\", priors[\"lmx\"][\"normal\"][0], priors[\"lmx\"][\"normal\"][1])\n",
    "            sigma_lmx = pm.HalfNormal(\"sigma_lmx\", priors[\"lmx\"][\"normal\"][2])\n",
    "            lmx_pred = pm.Normal(\"lmx_pred\", mu_lmx, sigma_lmx, observed=data[\"lmx\"].values)\n",
    "        else:\n",
    "            lmx_pred = pm.Uniform(\"lmx_pred\", 0, 40, observed=data[\"lmx\"].values)\n",
    "\n",
    "        # Likelihood(s)\n",
    "        lmx_imputed = pm.Normal(\n",
    "            \"lmx_imputed\",\n",
    "            alpha[0] + beta[0] * data[\"male\"] + beta[1] * climate_pred,\n",
    "            sigma[0],\n",
    "            observed=data[\"lmx\"].values,\n",
    "        )\n",
    "        climate_imputed = pm.Normal(\n",
    "            \"climate_imputed\",\n",
    "            alpha[1] + beta[2] * data[\"male\"],\n",
    "            sigma[1],\n",
    "            observed=data[\"climate\"].values,\n",
    "        )\n",
    "        empower_imputed = pm.Normal(\n",
    "            \"emp_imputed\",\n",
    "            alpha[2] + beta[3] * data[\"male\"] + beta[4] * climate_pred + beta[5] * lmx_pred,\n",
    "            sigma[2],\n",
    "            observed=data[\"empower\"].values,\n",
    "        )\n",
    "\n",
    "        idata = pm.sample_prior_predictive()\n",
    "        idata.extend(pm.sample(random_seed=120))\n",
    "        pm.sample_posterior_predictive(idata, extend_inferencedata=True)\n",
    "        return idata, model\n",
    "\n",
    "\n",
    "idata_uniform, model_uniform = make_model(priors, normal_pred_assumption=False)\n",
    "idata_normal, model_normal = make_model(priors, normal_pred_assumption=True)\n",
    "pm.model_to_graphviz(model_uniform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idata_normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we’ll inspect the parameter fits for our regression models and observe how they’re dependent on the prior specification in the imputation scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(idata_normal, var_names=[\"alphas\", \"beta\", \"sigmas\"], stat_focus=\"median\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(idata_uniform, var_names=[\"alphas\", \"beta\", \"sigmas\"], stat_focus=\"median\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how the choice of sampling distribution has induced different parameter estimates on the beta coefficients across our two models.\n",
    "The two imputations broadly agrees at the level of the parameters, but they meaningfully differ in their implications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_forest(\n",
    "    [idata_normal, idata_uniform],\n",
    "    var_names=[\"beta\"],\n",
    "    kind=\"ridgeplot\",\n",
    "    model_names=[\"Gaussian Sampling Distribution\", \"Uniform Sampling Distribution\"],\n",
    "    figsize=(10, 8),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This difference has downstream effects on the posterior predictive distribution. We can see here how the sampling distribution for the predictor terms influences the posterior predictive fits on our focal regression equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior Predictive Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_ppc(idata_uniform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_ppc(idata_normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the Posterior Predictive Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we estimated a number of likelihood terms in a single PyMC model context.\n",
    "These likelihoods constrained the hyper-parameters which determined the imputation values of the missing terms in the variables used as predictors in our focal regression equation for ```empower```.\n",
    "But we could also perform a more manual sequential imputation, where we model each of the subordinate regression equations seperately and extract the imputed values for each variable in turn and then run a simple regression on the imputed values for the focal regression equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We show here how to extract the imputed values for each of the regression equations and augment the observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_imputed(idata, data):\n",
    "    imputed_data = data.copy()\n",
    "    imputed_climate = az.extract(idata, group=\"posterior_predictive\", num_samples=1000)[\n",
    "        \"climate_imputed\"\n",
    "    ].mean(axis=1)\n",
    "    mask = imputed_data[\"climate\"].isnull()\n",
    "    imputed_data.loc[mask, \"climate\"] = imputed_climate.values[imputed_data[mask].index]\n",
    "\n",
    "    imputed_lmx = az.extract(idata, group=\"posterior_predictive\", num_samples=1000)[\n",
    "        \"lmx_imputed\"\n",
    "    ].mean(axis=1)\n",
    "    mask = imputed_data[\"lmx\"].isnull()\n",
    "    imputed_data.loc[mask, \"lmx\"] = imputed_lmx.values[imputed_data[mask].index]\n",
    "\n",
    "    imputed_emp = az.extract(idata, group=\"posterior_predictive\", num_samples=1000)[\n",
    "        \"emp_imputed\"\n",
    "    ].mean(axis=1)\n",
    "    mask = imputed_data[\"empower\"].isnull()\n",
    "    imputed_data.loc[mask, \"empower\"] = imputed_emp.values[imputed_data[mask].index]\n",
    "    assert imputed_data.isnull().sum().to_list() == [0, 0, 0, 0]\n",
    "    imputed_data.columns = [\"imputed_\" + col for col in imputed_data.columns]\n",
    "    return imputed_data\n",
    "\n",
    "\n",
    "imputed_data_uniform = get_imputed(idata_uniform, data)\n",
    "imputed_data_normal = get_imputed(idata_normal, data)\n",
    "imputed_data_normal.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the mean here to impute the expected value for each missing cell, but you could perform a kind of sensitivity analysis using the many plausible values in posterior predictive distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Imputed Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we’ll plot the imputed values against their observed values to show how the different sampling distributions impact the pattern of imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_uniform = pd.concat([imputed_data_uniform, data], axis=1)\n",
    "joined_normal = pd.concat([imputed_data_normal, data], axis=1)\n",
    "for col in [\"lmx\", \"empower\", \"climate\"]:\n",
    "    joined_uniform[col + \"_missing\"] = np.where(joined_uniform[col].isnull(), 1, 0)\n",
    "    joined_normal[col + \"_missing\"] = np.where(joined_normal[col].isnull(), 1, 0)\n",
    "\n",
    "\n",
    "def rand_jitter(arr):\n",
    "    stdev = 0.01 * (max(arr) - min(arr))\n",
    "    return arr + np.random.randn(len(arr)) * stdev\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(20, 8))\n",
    "axs = axs.flatten()\n",
    "ax = axs[0]\n",
    "ax1 = axs[1]\n",
    "ax2 = axs[2]\n",
    "\n",
    "## Derived from MV norm fit.\n",
    "z = multivariate_normal(\n",
    "    [lmx_mean, joined_uniform[\"imputed_empower\"].mean()], [[8.9, 5.4], [5.4, 19]]\n",
    ").pdf(joined_uniform[[\"imputed_lmx\", \"imputed_empower\"]])\n",
    "\n",
    "ax.scatter(\n",
    "    rand_jitter(joined_uniform[\"imputed_lmx\"]),\n",
    "    rand_jitter(joined_uniform[\"imputed_empower\"]),\n",
    "    c=joined_uniform[\"empower_missing\"],\n",
    "    cmap=cm.winter,\n",
    "    ec=\"black\",\n",
    "    s=50,\n",
    ")\n",
    "ax.set_title(\"Relationship between LMX and Empowerment \\n after Uniform Imputation\", fontsize=20)\n",
    "ax.tricontour(joined_uniform[\"imputed_lmx\"], joined_uniform[\"imputed_empower\"], z)\n",
    "ax.set_xlabel(\"Leader-Member-Exchange\")\n",
    "ax.set_ylabel(\"Empowerment\")\n",
    "\n",
    "\n",
    "custom_lines = [\n",
    "    Line2D([0], [0], color=cm.winter(0.0), lw=4),\n",
    "    Line2D([0], [0], color=cm.winter(0.9), lw=4),\n",
    "]\n",
    "ax.legend(custom_lines, [\"Observed\", \"Missing - Imputed Empowerment Values\"])\n",
    "\n",
    "z = multivariate_normal(\n",
    "    [lmx_mean, joined_normal[\"imputed_empower\"].mean()], [[8.9, 5.4], [5.4, 19]]\n",
    ").pdf(joined_normal[[\"imputed_lmx\", \"imputed_empower\"]])\n",
    "\n",
    "ax2.scatter(\n",
    "    rand_jitter(joined_normal[\"imputed_lmx\"]),\n",
    "    rand_jitter(joined_normal[\"imputed_empower\"]),\n",
    "    c=joined_normal[\"empower_missing\"],\n",
    "    cmap=cm.autumn,\n",
    "    ec=\"black\",\n",
    "    s=50,\n",
    ")\n",
    "ax2.set_title(\"Relationship between LMX and Empowerment \\n after Gaussian Imputation\", fontsize=20)\n",
    "ax2.tricontour(joined_normal[\"imputed_lmx\"], joined_normal[\"imputed_empower\"], z)\n",
    "ax2.set_xlabel(\"Leader-Member-Exchange\")\n",
    "ax2.set_ylabel(\"Empowerment\")\n",
    "custom_lines = [\n",
    "    Line2D([0], [0], color=cm.autumn(0.0), lw=4),\n",
    "    Line2D([0], [0], color=cm.autumn(0.9), lw=4),\n",
    "]\n",
    "ax2.legend(custom_lines, [\"Observed\", \"Missing - Imputed Empowerment Values\"])\n",
    "\n",
    "ax1.hist(\n",
    "    joined_normal[\"imputed_empower\"],\n",
    "    label=\"Gaussian Imputed Empowerment\",\n",
    "    bins=30,\n",
    "    color=\"slateblue\",\n",
    "    ec=\"black\",\n",
    ")\n",
    "ax1.hist(\n",
    "    joined_uniform[\"imputed_empower\"],\n",
    "    label=\"Uniform Imputed Empowerment\",\n",
    "    bins=30,\n",
    "    color=\"cyan\",\n",
    "    ec=\"black\",\n",
    ")\n",
    "ax1.hist(\n",
    "    joined_normal[\"empower\"], label=\"Observed Empowerment\", bins=30, color=\"magenta\", ec=\"black\"\n",
    ")\n",
    "ax1.legend()\n",
    "ax1.set_title(\"Imputed & Observed Empowerment\", fontsize=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimately our choice of sampling distribution leads to differently plausible imputations.\n",
    "The choice of which model to go with will driven by the assumptions which govern the reasons for missing-ness in our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Structures and Data Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our employee dataset has more fine-grained structure than we’ve examined so far.\n",
    "In particular there are 100 or so teams which make up our employee pool, and we might wonder to what degree the propensity for satisfaction or incomplete survey scores are due to the local team environments?\n",
    "Could this be a factor in our patterns of missing data?\n",
    "We’ll examine the reported empowerment scores by team and plot the regression lines by as predicted within each team by their reported ```lmx``` score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap = df_employee.pivot(\"employee\", \"team\", \"empower\").dropna(how=\"all\")\n",
    "heatmap = pd.concat(\n",
    "    [heatmap[~heatmap[col].isnull()][col].reset_index(drop=True) for col in heatmap.columns], axis=1\n",
    ")\n",
    "with pd.option_context(\"format.precision\", 2):\n",
    "    display(heatmap.style.background_gradient(cmap=\"Blues\"));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fits = []\n",
    "x = np.linspace(0, 20, 100)\n",
    "fig, ax = plt.subplots(figsize=(20, 7))\n",
    "for team in df_employee[\"team\"].unique():\n",
    "    temp = df_employee[df_employee[\"team\"] == team][[\"lmx\", \"empower\"]].dropna()\n",
    "    fit = np.polyfit(temp[\"lmx\"], temp[\"empower\"], 1)\n",
    "    y = fit[0] * x + fit[1]\n",
    "    fits.append(fit)\n",
    "    ax.plot(x, y, alpha=0.6)\n",
    "    ax.scatter(rand_jitter(temp[\"lmx\"]), rand_jitter(temp[\"empower\"]), color=\"black\", ec=\"white\")\n",
    "ax.set_title(\"Simple Regression fits by Team \\n Empower ~ LMX\", fontsize=20)\n",
    "ax.set_xlabel(\"Leader-Member-Exchange (LMX)\")\n",
    "ax.set_ylabel(\"Empowerment\")\n",
    "ax.set_ylim(0, 45);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is enough spread in the regression lines to at least suggest that there is a heterogenous relationship between empowerment and the work environment as we look across different teams, but limited observations in each team.\n",
    "This is a perfect use case for a hierarchical bayesian model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_idx, teams = pd.factorize(df_employee[\"team\"], sort=True)\n",
    "employee_idx, _ = pd.factorize(df_employee[\"employee\"], sort=True)\n",
    "coords = {\"team\": teams, \"employee\": np.arange(len(df_employee))}\n",
    "\n",
    "\n",
    "with pm.Model(coords=coords) as hierarchical_model:\n",
    "    # Priors\n",
    "    company_beta_lmx = pm.Normal(\"company_beta_lmx\", 0, 1)\n",
    "    company_beta_male = pm.Normal(\"company_beta_male\", 0, 1)\n",
    "    company_alpha = pm.Normal(\"company_alpha\", 20, 2)\n",
    "    team_alpha = pm.Normal(\"team_alpha\", 0, 1, dims=\"team\")\n",
    "    team_beta_lmx = pm.Normal(\"team_beta_lmx\", 0, 1, dims=\"team\")\n",
    "    sigma = pm.HalfNormal(\"sigma\", 4, dims=\"employee\")\n",
    "\n",
    "    # Imputed Predictors\n",
    "    mu_lmx = pm.Normal(\"mu_lmx\", 10, 5)\n",
    "    sigma_lmx = pm.HalfNormal(\"sigma_lmx\", 5)\n",
    "    lmx_pred = pm.Normal(\"lmx_pred\", mu_lmx, sigma_lmx, observed=df_employee[\"lmx\"].values)\n",
    "\n",
    "    # Combining Levels\n",
    "    alpha_global = pm.Deterministic(\"alpha_global\", company_alpha + team_alpha[team_idx])\n",
    "    beta_global_lmx = pm.Deterministic(\n",
    "        \"beta_global_lmx\", company_beta_lmx + team_beta_lmx[team_idx]\n",
    "    )\n",
    "    beta_global_male = pm.Deterministic(\"beta_global_male\", company_beta_male)\n",
    "\n",
    "    # Likelihood\n",
    "    mu = pm.Deterministic(\n",
    "        \"mu\",\n",
    "        alpha_global + beta_global_lmx * lmx_pred + beta_global_male * df_employee[\"male\"].values,\n",
    "    )\n",
    "\n",
    "    empower_imputed = pm.Normal(\n",
    "        \"emp_imputed\",\n",
    "        mu,\n",
    "        sigma,\n",
    "        observed=df_employee[\"empower\"].values,\n",
    "    )\n",
    "\n",
    "    idata_hierarchical = pm.sample_prior_predictive()\n",
    "    # idata_hierarchical.extend(pm.sample(random_seed=1200, target_accept=0.99))\n",
    "    idata_hierarchical.extend(\n",
    "        sample_blackjax_nuts(draws=20_000, random_seed=500, target_accept=0.99)\n",
    "    )\n",
    "    pm.sample_posterior_predictive(idata_hierarchical, extend_inferencedata=True)\n",
    "\n",
    "pm.model_to_graphviz(hierarchical_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idata_hierarchical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Convergence Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(\n",
    "    idata_hierarchical,\n",
    "    var_names=[\"company_alpha\", \"team_alpha\", \"company_beta_lmx\", \"team_beta_lmx\"],\n",
    "    kind=\"rank_vlines\",\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_energy(idata_hierarchical, figsize=(20, 7));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the Model Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = az.summary(\n",
    "    idata_hierarchical,\n",
    "    var_names=[\n",
    "        \"company_alpha\",\n",
    "        \"team_alpha\",\n",
    "        \"company_beta_lmx\",\n",
    "        \"company_beta_male\",\n",
    "        \"team_beta_lmx\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_ppc(\n",
    "    idata_hierarchical, var_names=[\"emp_imputed_observed\"], figsize=(20, 7), num_pp_samples=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heterogenous Patterns of Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as when we consider questions of causal inference and we attend to the confounding influence of local factors, this is also required when we do imputation.\n",
    "We show here a selection of team specific intercept terms which suggest that belonging to a particular team can shift your empowerment above or below the grand mean of the company level intercept term.\n",
    "These local effects of environment are what we seek to account for when imputing missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = az.plot_forest(\n",
    "    idata_hierarchical,\n",
    "    var_names=[\"team_beta_lmx\"],\n",
    "    coords={\"team\": [1, 20, 22, 30, 50, 70, 76, 80, 100]},\n",
    "    figsize=(20, 15),\n",
    "    kind=\"ridgeplot\",\n",
    "    combined=True,\n",
    "    ridgeplot_alpha=0.4,\n",
    "    hdi_prob=True,\n",
    ")\n",
    "ax[0].axvline(0)\n",
    "ax[0].set_title(\"Team Contribution to the marginal effect of LMX on Empowerment\", fontsize=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ability to capture this local variation impacts the pattern of imputed values too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_data = df_employee[[\"lmx\", \"empower\", \"climate\"]]\n",
    "\n",
    "imputed_lmx = az.extract(idata_hierarchical, group=\"posterior_predictive\", num_samples=1000)[\n",
    "    \"lmx_pred\"\n",
    "].mean(axis=1)\n",
    "mask = imputed_data[\"lmx\"].isnull()\n",
    "imputed_data.loc[mask, \"lmx\"] = imputed_lmx.values[imputed_data[mask].index]\n",
    "\n",
    "imputed_emp = az.extract(idata_hierarchical, group=\"posterior_predictive\", num_samples=1000)[\n",
    "    \"emp_imputed\"\n",
    "].mean(axis=1)\n",
    "mask = imputed_data[\"empower\"].isnull()\n",
    "imputed_data.loc[mask, \"empower\"] = imputed_emp.values[imputed_data[mask].index]\n",
    "imputed_data.columns = [\"imputed_\" + col for col in imputed_data.columns]\n",
    "joined = pd.concat([imputed_data, df_employee], axis=1)\n",
    "joined[\"check\"] = np.where(joined[\"empower\"].isnull(), 1, 0)\n",
    "\n",
    "mosaic = \"\"\"AAAABB\"\"\"\n",
    "fig, axs = plt.subplot_mosaic(mosaic, sharex=False, figsize=(20, 7))\n",
    "axs = [axs[k] for k in axs.keys()]\n",
    "axs[0].scatter(\n",
    "    joined[\"imputed_lmx\"],\n",
    "    joined[\"imputed_empower\"],\n",
    "    c=joined[\"check\"],\n",
    "    cmap=cm.winter,\n",
    "    ec=\"black\",\n",
    "    s=40,\n",
    ")\n",
    "\n",
    "z = multivariate_normal([10, joined[\"imputed_empower\"].mean()], [[8.9, 5.4], [5.4, 19]]).pdf(\n",
    "    joined[[\"imputed_lmx\", \"imputed_empower\"]]\n",
    ")\n",
    "axs[0].tricontour(joined[\"imputed_lmx\"], joined[\"imputed_empower\"], z)\n",
    "\n",
    "axs[1].hist(joined[\"imputed_empower\"], ec=\"black\", label=\"Imputed\", color=\"limegreen\", bins=30)\n",
    "axs[1].hist(joined[\"empower\"], ec=\"black\", label=\"observed\", color=\"blue\", bins=30)\n",
    "axs[1].set_title(\"Empowerment Distributions Imputed  \\n with Team Informed Estimates\", fontsize=20)\n",
    "axs[0].set_xlabel(\"Leader Member Exchange - LMX\")\n",
    "axs[0].set_ylabel(\"Empowerment\")\n",
    "axs[0].set_title(\"Empowerment Imputed \\n with Team Informed Estimates\", fontsize=20)\n",
    "axs[1].legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s clear from the hierarchical model that the team specific information has allowed us to impute a wider range of empowerment values with a broader spread as a function of ```lmx``` and ```male```.\n",
    "This is much more persuasive since all politics is local, and this latter model is informed by the conditions of work for each employee.\n",
    "As such, our hierarchical model is able to ascribe a more nuanced view of the probable empowerment values for the missing reports.\n",
    "The hierarchical imputation model “borrows information” in two ways (i) the individual team estimates are pulled toward the global estimates and (ii) the missing values are imputed with respect to our measures of the team dynamics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ve now seen multiple approaches to the imputation of missing data. We have focused on an example where the reason for the missing data is not immediately obvious given how different employees might very well have different reasons for under-specifying their relationship with management. However the techniques applied here are quite general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multivariate normal approaches to imputation works surprisingly well in many cases, but the more cutting edge approach is the sequential specification of chained equations. The Bayesian approach here is state of the art because we are quite free to use more than simple regression models as the component models for our imputation equations. For each equation we can be liberal in our choice of likelihood terms and the priors we allow over the sampling distributions. We can also add hierarchical structure to respect natural clusters in our data in so far as they constrain the patterns of missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This general point is important - the flexibility of the Bayesian approach can be tailored to the appropriate complexity of our theory about why our data is missing. Similar considerations apply to the estimation procedures involved in counterfactual inference. The more developed our theory for why the data is missing (why the world is as it is, and not another way), the more we need a flexible modelling framework to capture the subtleties of the theory. Bayesian modelling is a superb tool for this loop of theory construction and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n",
    "* Authored by Nathaniel Forde in February 2023 for pymc-examples #500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "* Craig Enders K. Applied Missing Data Analysis. The Guilford Press, 2022."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -n -u -v -iv -w -p pytensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nathaniel Forde . \"Bayesian Missing Data Imputation\". In: PyMC Examples. Ed. by PyMC Team. DOI: 10.5281/zenodo.5654871"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
